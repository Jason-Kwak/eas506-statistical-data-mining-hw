text(fit, use.n = TRUE,cex = .8)
# Prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #4
plot(fit$cptable[,4],main = "Cp for model selection", ylab = "CV error")
prune_fit<-prune(fit, cp=fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single tree
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat<-as.numeric(my_pred)
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
################################################
## train  data count on prune_fit tree
##############################################
set.seed(123123)
# Count number of training obeservations in nodes
nodes_position <- prune_fit
nodes_position$frame$yval = as.numeric(rownames(nodes_position$frame))
train_nodes <- predict(nodes_position, trainDF, type = "vector")
trainNodeDf = data.frame(rowNum = c(1:length(train_nodes)),train_nodes)
trainNodeData = data.frame(aggregate(rowNum~train_nodes,data = trainNodeDf,FUN = length))
wineTreePred_test <- predict(prune_fit, newdata = testDF, type = 'class')
wineTreePred_train <- predict(prune_fit, newdata = trainDF, type= 'class')
test_err <- mean(wineTreePred_test != prune_fit$V1)
train_err <- mean(wineTreePred_train != prune_fit$V1)
test_err
train_err
testNodeData
trainNodeData
##########################################################
#Statistical Data Mining I
#Homework 4 question -2
#############################################################
rm(list = ls())
install.packages("ISLR")
library("ISLR")
install.packages("caret")
library(caret)
library(MASS)
install.packages ("e1071")
library("e1071")
library(leaps)
install.packages("caTools")
library(caTools)
library(rpart)
install.packages("randomForest")
library(randomForest)
install.packages("gbm")
library(gbm)
library(geneplotter)
library(ISLR)
#########################################
# Create a training and test set
#########################################
data1 <- read.csv("wine.csv")
data2<-data.frame(data1)
head(data2)
data2<-data2[,-c(2,7,8)]
dim(data2) #177 11
ind<-sample.split(Y = data2,SplitRatio = 0.7)
trainDF<-data2[ind,]
dim(trainDF) #112 11
testDF<-data2[!ind,]
dim(testDF) #65 11
train_y<-trainDF$X1
train_x<-data.frame(trainDF$X1.71,trainDF$X2.43,trainDF$X15.6,trainDF$X127,trainDF$X.28,trainDF$X2.29,trainDF$X5.64,trainDF$X1.04,trainDF$X3.92,trainDF$X1065)
head(train_x)
test_y<-testDF$X1
test_x<-data.frame(testDF$X1.71,testDF$X2.43,testDF$X15.6,testDF$X127,testDF$X.28,testDF$X2.29,testDF$X5.64,testDF$X1.04,testDF$X3.92,testDF$X1065)
head(test_x)
#X1=as.factor(X1)
######################################
###########################################
#Single tree
##########################################
set.seed(12406)
model.control<-rpart.control(minsplit = 5,xval=10, cp=0)
fit<-rpart(X1~.,data = trainDF, method = "class", control = model.control)
plot(fit, main = " Training before pruning")
text(fit,use.n=TRUE,cex = .8)
#prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #5
plot(fit$cptable[,4],main = "cp for model selection", ylab = "cv error")
saveeps("test")
prune_fit<-prune(fit, cp=fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single tree
my_pred<-predict(prune_fit,newdata = testDF, type = "class")
my_pred
y_hat<-as.numeric(my_pred)
y_hat
misclass_tree<-sum(abs(test_y-y_hat))/length(y_hat)
misclass_tree
#0.10769
################################################
##train  data count on prune_fit tree
##############################################
set.seed(98765)
#Count number of train data in nodes
nodes_position <- prune_fit
nodes_position$frame$yval = as.numeric(rownames(nodes_position$frame))
train_nodes <- predict(nodes_position, trainDF, type="vector")
trainNodeDf = data.frame(rowNum = c(1:length(train_nodes)),train_nodes)
trainNodeData = data.frame(aggregate(rowNum~train_nodes,data = trainNodeDf,FUN = length))
wineTreePred_test <- predict(prune_fit, newdata = testDF,type = 'class')
wineTreePred_train <- predict(prune_fit, newdata = trainDF, type='class')
test_err <- mean(wineTreePred_test != prune_fit$V1)
train_err <- mean(wineTreePred_train != prune_fit$V1)
test_err
train_err
testNodeData
trainNodeData
#########################################
###### test data count on prune_fit tree
#########################################
test_nodes <- predict(nodes_position, testDF, type="vector")
testNodeDf = data.frame(rowNum = c(1:length(test_nodes)),test_nodes)
testNodeData = data.frame(aggregate(rowNum~test_nodes,data = testNodeDf,FUN = length))
testNodeData
install.packages("ISLR")
install.packages("caret")
install.packages("e1071")
install.packages("caTools")
install.packages("randomForest")
install.packages("gbm")
install.packages("e1071")
install.packages("e1071")
set.seed(98765)
#Count number of train data in nodes
nodes_position <- prune_fit
nodes_position$frame$yval = as.numeric(rownames(nodes_position$frame))
train_nodes <- predict(nodes_position, trainDF, type="vector")
trainNodeDf = data.frame(rowNum = c(1:length(train_nodes)),train_nodes)
trainNodeData = data.frame(aggregate(rowNum~train_nodes,data = trainNodeDf,FUN = length))
wineTreePred_test <- predict(prune_fit, newdata = testDF,type = 'class')
wineTreePred_train <- predict(prune_fit, newdata = trainDF, type='class')
test_err <- mean(wineTreePred_test != prune_fit$V1)
train_err <- mean(wineTreePred_train != prune_fit$V1)
test_err
train_err
testNodeData
trainNodeData
#########################################
###### test data count on prune_fit tree
#########################################
test_nodes <- predict(nodes_position, testDF, type="vector")
testNodeDf = data.frame(rowNum = c(1:length(test_nodes)),test_nodes)
testNodeData = data.frame(aggregate(rowNum~test_nodes,data = testNodeDf,FUN = length))
testNodeData
set.seed(12406)
model.control<-rpart.control(minsplit = 5,xval=10, cp=0)
fit<-rpart(X1~.,data = trainDF, method = "class", control = model.control)
plot(fit, main = " Training before pruning")
text(fit,use.n=TRUE,cex = .8)
#prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #5
plot(fit$cptable[,4],main = "cp for model selection", ylab = "cv error")
saveeps("test")
prune_fit<-prune(fit, cp=fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single tree
my_pred<-predict(prune_fit,newdata = testDF, type = "class")
my_pred
y_hat<-as.numeric(my_pred)
y_hat
misclass_tree<-sum(abs(test_y-y_hat))/length(y_hat)
misclass_tree
#0.10769
################################################
##train  data count on prune_fit tree
##############################################
set.seed(98765)
#Count number of train data in nodes
nodes_position <- prune_fit
nodes_position$frame$yval = as.numeric(rownames(nodes_position$frame))
train_nodes <- predict(nodes_position, trainDF, type="vector")
trainNodeDf = data.frame(rowNum = c(1:length(train_nodes)),train_nodes)
trainNodeData = data.frame(aggregate(rowNum~train_nodes,data = trainNodeDf,FUN = length))
wineTreePred_test <- predict(prune_fit, newdata = testDF,type = 'class')
wineTreePred_train <- predict(prune_fit, newdata = trainDF, type='class')
test_err <- mean(wineTreePred_test != prune_fit$V1)
train_err <- mean(wineTreePred_train != prune_fit$V1)
test_err
train_err
testNodeData
trainNodeData
#########################################
###### test data count on prune_fit tree
#########################################
test_nodes <- predict(nodes_position, testDF, type="vector")
testNodeDf = data.frame(rowNum = c(1:length(test_nodes)),test_nodes)
testNodeData = data.frame(aggregate(rowNum~test_nodes,data = testNodeDf,FUN = length))
testNodeData
# Clear the memory
rm(list = ls())
# Set working directory
setwd("~/Course Material/EAS506 - Stat Data Mining/Homework/Homework 4/Question 2")
#install.packages("caret")
#install.packages("ISLR")
#install.packages("MASS")
#install.packages ("e1071")
#install.packages("caTools")
#install.packages("leaps")
#install.packages("rpart")
#install.packages("randomForest")
#install.packages("gbm")
#install.packages("geneplotter")
library("caret")
library("ISLR")
library("MASS")
library("e1071")
library("caTools")
library("leaps")
library("rpart")
library("randomForest")
library("gbm")
library("geneplotter")
#########################################
# Create a training and test set
#########################################
data1 <- read.csv("wine.csv")
data2 <- data.frame(data1)
wine <-data2[,-c(2,7,8)]
dim(wine) #177 11
train <-sample(1:NROW(wine),NROW(wine)*0.70)
test <- setdiff(1:NROW(wine),train)
trainDF = wine[train, ]
dim(trainDF) #123 11
testDF = wine[-train, ]
dim(testDF) #54 11
train_y <- trainDF[1]
dim(train_y) #123 1
train_x <- trainDF[,-1]
dim(train_x) #123 10
head(train_x)
test_y <- testDF[1]
dim(test_y) #54 1
test_x <- testDF[,-1]
dim(test_x) #54 10
head(test_x)
###########################################
# Single tree
##########################################
set.seed(123123)
model.control <- rpart.control(minsplit = 5,xval=10, cp=0)
fit <- rpart(X1~.,data = trainDF, method = "class", control = model.control)
x11()
plot(fit, main = " Training (Pre-pruning)")
text(fit, use.n = TRUE,cex = .8)
# Prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #4
plot(fit$cptable[,4],main = "Cp for model selection", ylab = "CV error")
prune_fit<-prune(fit, cp=fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single tree
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat<-as.numeric(my_pred)
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
################################################
## train  data count on prune_fit tree
##############################################
set.seed(123123)
# Count number of training obeservations in nodes
nodes_position <- prune_fit
nodes_position$frame$yval = as.numeric(rownames(nodes_position$frame))
train_nodes <- predict(nodes_position, trainDF, type = "vector")
trainNodeDf = data.frame(rowNum = c(1:length(train_nodes)),train_nodes)
trainNodeData = data.frame(aggregate(rowNum~train_nodes,data = trainNodeDf,FUN = length))
wineTreePred_test <- predict(prune_fit, newdata = testDF, type = 'class')
wineTreePred_train <- predict(prune_fit, newdata = trainDF, type= 'class')
test_err <- mean(wineTreePred_test != prune_fit$V1)
train_err <- mean(wineTreePred_train != prune_fit$V1)
test_err
train_err
testNodeData
trainNodeData
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
set.seed(123123)
model.control <- rpart.control(minsplit = 5,xval=10, cp=0)
fit <- rpart(X1~.,data = trainDF, method = "class", control = model.control)
x11()
plot(fit, main = " Training (Pre-pruning)")
text(fit, use.n = TRUE,cex = .8)
# Prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #4
plot(fit$cptable[,4],main = "Cp for model selection", ylab = "CV error")
prune_fit<-prune(fit, cp=fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single tree
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat<-as.numeric(my_pred)
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
############################################################################################
##
## HW4 Question 2
##
## Author: Paul M Girdler
## Created: November 25, 2018
##
############################################################################################
# Clear the memory
rm(list = ls())
# Set working directory
setwd("~/Course Material/EAS506 - Stat Data Mining/Homework/Homework 4/Question 2")
#install.packages("caret")
#install.packages("ISLR")
#install.packages("MASS")
#install.packages ("e1071")
#install.packages("caTools")
#install.packages("leaps")
#install.packages("rpart")
#install.packages("randomForest")
#install.packages("gbm")
#install.packages("geneplotter")
library("caret")
library("ISLR")
library("MASS")
library("e1071")
library("caTools")
library("leaps")
library("rpart")
library("randomForest")
library("gbm")
library("geneplotter")
#########################################
# Create a training and test set
#########################################
data1 <- read.csv("wine.csv")
data2 <- data.frame(data1)
wine <-data2[,-c(2,7,8)]
dim(wine) #177 11
train <-sample(1:NROW(wine),NROW(wine)*0.70)
test <- setdiff(1:NROW(wine),train)
trainDF = wine[train, ]
dim(trainDF) #123 11
testDF = wine[-train, ]
dim(testDF) #54 11
train_y <- trainDF[1]
dim(train_y) #123 1
train_x <- trainDF[,-1]
dim(train_x) #123 10
head(train_x)
test_y <- testDF[1]
dim(test_y) #54 1
test_x <- testDF[,-1]
dim(test_x) #54 10
head(test_x)
###########################################
# Single tree
##########################################
set.seed(123123)
model.control <- rpart.control(minsplit = 5,xval=10, cp=0)
fit <- rpart(X1~.,data = trainDF, method = "class", control = model.control)
x11()
plot(fit, main = " Training (Pre-pruning)")
text(fit, use.n = TRUE,cex = .8)
# Prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #4
plot(fit$cptable[,4],main = "Cp for model selection", ylab = "CV error")
prune_fit<-prune(fit, cp=fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single tree
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat<-as.numeric(my_pred)
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
set.seed(123123)
model.control <- rpart.control(minsplit = 5,xval=10, cp=0)
fit <- rpart(X1~.,data = trainDF, method = "class", control = model.control)
x11()
plot(fit, main = " Training (Pre-pruning)")
text(fit, use.n = TRUE,cex = .8)
# Prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #6
plot(fit$cptable[,4],main = "Cp for model selection", ylab = "CV error")
prune_fit<-prune(fit, cp=fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single tree
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat<-as.numeric(my_pred)
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
dim(test_x) #54 10
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat<-as.numeric(my_pred)
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat<-as.numeric(my_pred)
misclass_tree <- mean(abs(test_y - y_hat))
misclass_tree #0.0185
test_y
y_hat
length(test_y)
length(y_hat)
############################################################################################
##
## HW4 Question 2
##
## Author: Paul M Girdler
## Created: November 25, 2018
##
############################################################################################
# Clear the memory
rm(list = ls())
# Set working directory
setwd("~/Course Material/EAS506 - Stat Data Mining/Homework/Homework 4/Question 2")
#install.packages("caret")
#install.packages("ISLR")
#install.packages("MASS")
#install.packages ("e1071")
#install.packages("caTools")
#install.packages("leaps")
#install.packages("rpart")
#install.packages("randomForest")
#install.packages("gbm")
#install.packages("geneplotter")
library("caret")
library("ISLR")
library("MASS")
library("e1071")
library("caTools")
library("leaps")
library("rpart")
library("randomForest")
library("gbm")
library("geneplotter")
#########################################
# Create a training and test set
#########################################
data1 <- read.csv("wine.csv")
data2 <- data.frame(data1)
wine <-data2[,-c(2,7,8)]
dim(wine) #177 11
train <-sample(1:NROW(wine),NROW(wine)*0.70)
test <- setdiff(1:NROW(wine),train)
trainDF = wine[train, ]
dim(trainDF) #123 11
testDF = wine[-train, ]
dim(testDF) #54 11
train_y <- trainDF[1]
train_y <- as.numeric(train_y)
dim(train_y) #123 1
train_x <- trainDF[,-1]
dim(train_x) #123 10
head(train_x)
test_y <- testDF[1]
test_y <- as.numeric(test_y)
dim(test_y) #54 1
test_x <- testDF[,-1]
dim(test_x) #54 10
head(test_x)
set.seed(123123)
model.control <- rpart.control(minsplit = 5, xval=10, cp=0)
fit <- rpart(X1~.,data = trainDF, method = "class", control = model.control)
x11()
plot(fit, main = " Training (Pre-pruning)")
text(fit, use.n = TRUE,cex = .8)
# Prune the tree
min_cp = which.min(fit$cptable[,4])
min_cp #6
plot(fit$cptable[,4],main = "Cp for model selection", ylab = "CV error")
prune_fit <- prune(fit, cp = fit$cptable[min_cp,1])
plot(prune_fit, main = "Training after pruning")
text(prune_fit,use.n=TRUE , cex=.8)
#test error of single pruned tree
my_pred <- predict(prune_fit, newdata = testDF, type = "class")
y_hat <- as.numeric(my_pred)
misclass_tree <- sum(abs(test_y - y_hat))/length(y_hat)
misclass_tree #0.0185
################################################
## train  data count on prune_fit tree
##############################################
set.seed(123123)
# Count number of training obeservations in nodes
nodes_position <- prune_fit
nodes_position$frame$yval = as.numeric(rownames(nodes_position$frame))
train_nodes <- predict(nodes_position, trainDF, type = "vector")
trainNodeDf = data.frame(rowNum = c(1:length(train_nodes)), train_nodes)
trainNodeData = data.frame(aggregate(rowNum~train_nodes,data = trainNodeDf,FUN = length))
wineTreePred_test <- predict(prune_fit, newdata = testDF, type = 'class')
wineTreePred_train <- predict(prune_fit, newdata = trainDF, type= 'class')
test_err <- mean(wineTreePred_test != prune_fit$V1)
train_err <- mean(wineTreePred_train != prune_fit$V1)
test_err
train_err
testNodeData
trainNodeData
#########################################
# test data count on prune_fit tree
#########################################
test_nodes <- predict(nodes_position, testDF, type="vector")
testNodeDf = data.frame(rowNum = c(1:length(test_nodes)),test_nodes)
testNodeData = data.frame(aggregate(rowNum~test_nodes,data = testNodeDf,FUN = length))
testNodeData
